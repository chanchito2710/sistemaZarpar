# =====================================================
# CONFIGURACIÓN NGINX - ANTI-SEO
# Prevenir indexación por motores de búsqueda
# =====================================================

# Agregar estos headers a tu configuración de Nginx en PRODUCCIÓN

# =====================================================
# 1. HEADERS ANTI-SEO (Agregar en location /)
# =====================================================

# Prevenir indexación
add_header X-Robots-Tag "noindex, nofollow, noarchive, nosnippet, noimageindex" always;

# Cache control estricto
add_header Cache-Control "no-cache, no-store, must-revalidate, private" always;
add_header Pragma "no-cache" always;
add_header Expires "0" always;

# Ocultar información del servidor
server_tokens off;
more_clear_headers Server;
more_clear_headers X-Powered-By;

# =====================================================
# 2. BLOQUEAR ACCESO A ARCHIVOS SENSIBLES
# =====================================================

# Bloquear robots.txt (ya responde con Disallow: / pero mejor asegurar)
location = /robots.txt {
    add_header Content-Type text/plain;
    add_header X-Robots-Tag "noindex, nofollow" always;
    return 200 "User-agent: *\nDisallow: /\n";
}

# Bloquear sitemap.xml (no deberías tenerlo)
location = /sitemap.xml {
    return 404;
}

# Bloquear cualquier sitemap
location ~* /sitemap.*\.(xml|txt)$ {
    return 404;
}

# Bloquear archivos de configuración
location ~ /\. {
    deny all;
    return 404;
}

# Bloquear archivos sensibles
location ~* \.(env|git|gitignore|htaccess|htpasswd|ini|log|sh|sql|conf|config)$ {
    deny all;
    return 404;
}

# =====================================================
# 3. BLOQUEAR USER AGENTS DE BOTS CONOCIDOS
# =====================================================

# Lista de bots a bloquear
map $http_user_agent $bad_bot {
    default 0;
    
    # Motores de búsqueda principales
    ~*googlebot 1;
    ~*bingbot 1;
    ~*slurp 1;
    ~*duckduckbot 1;
    ~*baiduspider 1;
    ~*yandexbot 1;
    ~*sogou 1;
    ~*exabot 1;
    
    # Bots de redes sociales
    ~*facebookexternalhit 1;
    ~*facebot 1;
    ~*twitterbot 1;
    ~*linkedinbot 1;
    
    # Scrapers y crawlers
    ~*ia_archiver 1;
    ~*archive.org_bot 1;
    ~*ahrefsbot 1;
    ~*semrushbot 1;
    ~*mj12bot 1;
    ~*dotbot 1;
    ~*blexbot 1;
    ~*dataprovider 1;
    
    # Agregadores de contenido
    ~*feedfetcher 1;
    ~*feedburner 1;
    ~*feedly 1;
}

# Aplicar bloqueo en location principal
# NOTA: Descomentar solo si quieres bloquear COMPLETAMENTE los bots
# (puede afectar previews en redes sociales)
# if ($bad_bot) {
#     return 403 "Bot bloqueado";
# }

# =====================================================
# 4. EJEMPLO DE CONFIGURACIÓN COMPLETA
# =====================================================

# server {
#     listen 80;
#     server_name tu-dominio.com;
# 
#     # Ocultar versión de Nginx
#     server_tokens off;
# 
#     # Anti-SEO Headers
#     add_header X-Robots-Tag "noindex, nofollow, noarchive" always;
#     add_header Cache-Control "no-cache, no-store, must-revalidate" always;
#     add_header Pragma "no-cache" always;
# 
#     # Frontend (archivos estáticos)
#     location / {
#         root /var/www/zarpar/dist;
#         try_files $uri $uri/ /index.html;
#         
#         # Headers anti-SEO en archivos estáticos también
#         add_header X-Robots-Tag "noindex, nofollow" always;
#     }
# 
#     # Backend API
#     location /api {
#         proxy_pass http://localhost:3456;
#         proxy_http_version 1.1;
#         proxy_set_header Upgrade $http_upgrade;
#         proxy_set_header Connection 'upgrade';
#         proxy_set_header Host $host;
#         proxy_cache_bypass $http_upgrade;
#         proxy_set_header X-Real-IP $remote_addr;
#         proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
#         
#         # Headers anti-SEO en API también
#         add_header X-Robots-Tag "noindex, nofollow" always;
#     }
# 
#     # Bloquear robots.txt personalizado
#     location = /robots.txt {
#         add_header Content-Type text/plain;
#         return 200 "User-agent: *\nDisallow: /\n";
#     }
# 
#     # Bloquear acceso a archivos sensibles
#     location ~ /\. {
#         deny all;
#         return 404;
#     }
# }

# =====================================================
# 5. INSTRUCCIONES DE USO
# =====================================================

# 1. Copiar la configuración necesaria a tu archivo de Nginx
#    Ejemplo: /etc/nginx/sites-available/zarpar
#
# 2. Verificar la sintaxis:
#    sudo nginx -t
#
# 3. Recargar Nginx:
#    sudo systemctl reload nginx
#
# 4. Verificar headers:
#    curl -I https://tu-dominio.com
#    Deberías ver: X-Robots-Tag: noindex, nofollow...

# =====================================================
# 6. VERIFICACIÓN
# =====================================================

# Verificar que los headers están aplicados:
# curl -I https://tu-dominio.com | grep -i robot
# curl -I https://tu-dominio.com | grep -i cache

# =====================================================
# IMPORTANTE
# =====================================================

# ⚠️ ESTAS CONFIGURACIONES SON PARA PRODUCCIÓN
# ⚠️ Asegúrate de que tu dominio NO esté en tu sitemap
# ⚠️ Si usas Cloudflare, configura también los headers ahí
# ⚠️ Verifica que robots.txt responda con "Disallow: /"

